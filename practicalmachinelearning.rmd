---
output: word_document
---
# Assessment : How to exercise efficiently ? 

Background
========

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

Library
========
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

Random Number Generation
========
Integer vector, containing the random number generator (RNG) state for random number generation in R
```{r}
set.seed(12345)
```

Dataset
========

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

Download both training dataset :-


```{r}
curdir <-getwd()
file.url<-'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
download.file(file.url,destfile=paste(curdir,'/pml-training.csv',sep=""))

curdir <-getwd()
file.url<-'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(file.url,destfile=paste(curdir,'/pml-testing.csv',sep=""))

```

Load both dataset and change the missing value "#DIV/0!" to "NA" . 
```{r}
training <-read.csv(paste(curdir,'/pml-training.csv',sep=""),na.strings=c("NA","#DIV/0!",""))
testing <-read.csv(paste(curdir,'/pml-testing.csv',sep=""), na.strings=c("NA","#DIV/0!", ""))
```

Delete column which has missing values.
```{r}
training<-training[,colSums(is.na(training)) == 0]
testing <-testing[,colSums(is.na(testing)) == 0]
```

Cheking the dimension of training and test dataset :-

```{r}
dim(training)
dim(testing)
```

Checking the columns which have all missing values

```{r}
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```

We remove 6 of the variables which is irrelevant like :-

a)  user_name

b)  raw_timestamp_part_1

c)  raw_timestamp_part_2

d)  cvtd_timestamp

e)  new_window

f)  num_window 


which resides on the column 1-7.

```{r}
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```

Check again the dimension

```{r}
dim(training)
dim(testing)
```

Now we obtain the several rows to preview

```{r}
head(training)
head(testing)
```

In order to run cross-validation , the training dataset need to partition into 2 sets . We set the 1st partition for training dataset to 75% and test dataset to 25%. Training dataset contains 53 variables with 19622 obs and test dataset contains 53 variables with 20 obs.

This will do the randomize sub-sampling without replacement 


```{r}
chunks <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
chunks_training <- training[chunks, ]; 
chunks_testing <- training[-chunks, ]
dim(chunks_training); 
dim(chunks_testing);

```

Visualization
============
We try to plot into the histogram to see the trending frequency of each sub-training & test dataset by comparing with each other. The variable classe contains 5 levels which is A,B,C,D & E


```{r}
plot(chunks_training$classe, col="lightgreen", main="Bar Plot Classe vs. Frequency ", xlab="Classe", ylab="Frequency")
```

The graph above shows that A ~ 4000x occurrences is most frequent while D is the lest frequent ~ 2500x occurrences


Decision Tree
============
Decision Tree machine learning algorithm as a support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.

```{r}
Fit_Model_1 <- rpart(classe ~ ., data=chunks_training, method="class")
```

Displays the (Complexity) cp table for fitted model  .

```{r}
printcp(Fit_Model_1)
```

To visualize the decision tree , we use this fancyRpartPlot command below :-

```{r}
fancyRpartPlot(Fit_Model_1,main="Classification Tree")
```

Green nodes represent individuals classified by the tree as A, blue nodes are those classified as B and orange nodes are classified as C. The gradient is a visual representation of the three numbers in the middle of the nodes: bearing in mind that levels of a factor are by default in alphabetical order, the first of these three numbers is the proportion of individuals in that node that were actually classified as the first level, (A), in train_part ; the second number is the proportion that were actually classified as B, and the third the proportion that were C.


```{r}
rpart.plot(Fit_Model_1,main="Classification Tree",extra=102, under=TRUE, faclen=0)
```

Now we predict the fit model for test dataset . 

```{r}
Prediction_Model1 <- predict(Fit_Model_1, chunks_testing, type = "class")
```

Confusion Matrix
============
Confusion matrix, also known as a contingency table or an error matrix , is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice-versa).

```{r}
confusionMatrix(Prediction_Model1, chunks_testing$classe)
```

Random Forest
============
Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set.

```{r}
Fit_Model_2 <- randomForest(classe ~. , data=chunks_training)
```

Now we predict the fit model for test dataset . 

```{r}
Prediction_Model2 <- predict(Fit_Model_2, chunks_testing, type = "class")
```

Below is the confusion matrix of the test results 

```{r}
confusionMatrix(Prediction_Model2, chunks_testing$classe)
```


Conclusion
============
From the machine learning method above , the cross validation accuracy of the Decision Tree is ~ 66.17%  and the Random Forest is ~ 99.3% which is better and the sample error rate rather small around ~ 0.07% .

```{r}
Final_Prediction <- predict(Fit_Model_2, testing, type = "class")
```

Random Forests generally needs larger number of instances to work its randomization concept well and generalize to the novel data. In addition, in one way or another, random forests works with combination of some kind of soft linear boundaries at the decision surface

Prediction files generator for assignment submission code
============

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(Final_Prediction)
```

Reference
============
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3lj0hACeI





